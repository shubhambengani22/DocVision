{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am going to use TF-IDF to find the most important sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF is abbreviation for Term Frequency which is the frequency of a word appearing in a document divided by the total number of all words in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tf-nlp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets say we are reading an article about Machine Learning and/or Deep Learning, so these 4 words will be more commonly found in the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quite often, we would want to build a dictionary (hashmap) of term frequencies alongside the term. Like {word: term frequency of that word} and then iterate through this dictionary to find out which word appears the most times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "#### -> Removal of stopwords - Stopwords are the words such as \"and\", \"the\", etc. which are present in the document but will make very less impact on the task. Hence, we will have to remove it.\n",
    "#### -> Case of the words - Now, two words, \"Machine\", and \"machine\" are same words but due to the case difference these will be treated as two different words, hence we will convert them to only one case, either lower or upper.\n",
    "#### -> Removing comma, fullstops, or any punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from PIL import Image\n",
    "import sys\n",
    "import pyocr\n",
    "import pyocr.builders\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = \" \".join([word for word in doc.split(\" \") if word not in stopwords.words('english')])\n",
    "    doc = doc.replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \").replace(\"-\", \"\").replace('\"', \"\")\n",
    "    temp_sentences = doc.strip().split(\".\")\n",
    "    sentences = []\n",
    "    for sentence in temp_sentences:\n",
    "        sentences.append(\"\".join([char for char in sentence if char not in string.punctuation]).strip())\n",
    "    doc = \"\".join([char for char in doc if char not in string.punctuation])\n",
    "    return doc, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromImage(filename):\n",
    "    doc = tool.image_to_string(\n",
    "        Image.open(filename),\n",
    "        lang=lang,\n",
    "        builder=pyocr.builders.TextBuilder()\n",
    "    )\n",
    "    # txt is a Python string\n",
    "\n",
    "    word_boxes = tool.image_to_string(\n",
    "        Image.open(filename),\n",
    "        lang=\"eng\",\n",
    "        builder=pyocr.builders.WordBoxBuilder()\n",
    "    )\n",
    "    return doc, word_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWords(doc):\n",
    "    freq = {}\n",
    "    for word in doc.split(\" \"):\n",
    "        if word not in freq:\n",
    "            freq[word] = 1\n",
    "        else:\n",
    "            freq[word] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def termFrequency(doc):\n",
    "    #Get the data in the required format and the sentences\n",
    "    clean_doc, sentences = preprocess(doc)\n",
    "    unique_words = len(set(clean_doc.split(\" \")))\n",
    "        \n",
    "    #Get the frequency dictionary\n",
    "    frequency = countWords(clean_doc)\n",
    "    \n",
    "    #Calculate TF according to the formula\n",
    "    tf_dict = {}\n",
    "    for word in frequency:\n",
    "        if word not in tf_dict:\n",
    "            tf_dict[word] = frequency[word]/unique_words\n",
    "    \n",
    "    #Get the TF value for the whole sentence\n",
    "    sentence_tf = {}\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        tf_sum = 0\n",
    "        for word in sentence.split(\" \"):\n",
    "            tf_sum += tf_dict[word]\n",
    "        sentence_tf[sentence] = (tf_sum, i)\n",
    "        i += 1\n",
    "    return sentence_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want a sentence that is both rare, unique and contains keywords common in the article. This is where inverse document frequency comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"idf-nlp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF used over many documents, whereas TF is built for one document. We can decide what a document is. In this piece of text, each sentence is its own document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverseDocumentFrequency(doc):\n",
    "    #Get the data in the required format and the sentences\n",
    "    clean_doc, sentences = preprocess(doc)\n",
    "    unique_words = len(set(clean_doc.split(\" \")))\n",
    "    \n",
    "    #Since we are treating each sentence as a separate document, the number of documents will be the length of sentences array\n",
    "    numDocs = len(sentences)\n",
    "    \n",
    "    frequency = countWords(clean_doc)\n",
    "    \n",
    "    #We have to count the words in each sentence, since each sentence is a document\n",
    "    wordCountSentences = []\n",
    "    for sentence in sentences:\n",
    "        wordCountSentences.append(countWords(sentence))\n",
    "    \n",
    "    #Getting the inverse document frequency\n",
    "    IDF = {}\n",
    "    for word in list(set(clean_doc.split(\" \"))):\n",
    "        temp_count = 0\n",
    "        for sentence in wordCountSentences:\n",
    "            if word in sentence:\n",
    "                temp_count += 1\n",
    "        IDF[word] = math.log10(len(sentences)/temp_count)\n",
    "        \n",
    "    #Getting IDF values for sentences\n",
    "    IDF_sentences = {}\n",
    "    i = 0\n",
    "    for i in range(len(sentences)):\n",
    "        words = sentences[i].split(\" \")\n",
    "        temp_add = 0.0\n",
    "        words_no_stop_words = preprocess(sentences[i])\n",
    "        for word in words:\n",
    "            if word.lower() in IDF:\n",
    "                temp_add += IDF[word.lower()]\n",
    "        IDF_sentences[sentences[i]] = (temp_add/len(words_no_stop_words), i)\n",
    "        i += 1\n",
    "    \n",
    "    return IDF_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(doc):\n",
    "    TF = termFrequency(doc)\n",
    "    IDF = inverseDocumentFrequency(doc)\n",
    "    \n",
    "    TFIDF_dict = {}\n",
    "    for key in TF:\n",
    "        if key in IDF:\n",
    "            TFIDF_dict[key] = (TF[key][0] * IDF[key][0], TF[key][1])\n",
    "                    \n",
    "    max1 = 0.0;\n",
    "    max2 = 0.0;\n",
    "    max3 = 0.0;\n",
    "\n",
    "    max1Sent = \"\";\n",
    "    max2Sent = \"\";\n",
    "    max3Sent = \"\";\n",
    "    \n",
    "    sorted_TFIDF = sorted(TFIDF_dict.items(), key=lambda e: e[1], reverse=True)\n",
    "    \n",
    "    #finding the top 3 sentences in TFidfDict\n",
    "    '''max_sentences = []\n",
    "    for (key, value) in TFIDF_dict.items():\n",
    "        if TFIDF_dict[key][0] > max1:\n",
    "            max1 = TFIDF_dict[key][0]\n",
    "            max1Sent = key;\n",
    "            max_sentences.append((max1Sent, TFIDF_dict[key][1], TFIDF_dict[key][0]))\n",
    "        elif TFIDF_dict[key][0] > max2 and TFIDF_dict[key][0] < max1:\n",
    "            max2 = TFIDF_dict[key][0]\n",
    "            max2Sent = key;\n",
    "            max_sentences.append((max2Sent, TFIDF_dict[key][1], TFIDF_dict[key][0]))\n",
    "        elif TFIDF_dict[key][0] > max3 and TFIDF_dict[key][0] < max2 and TFIDF_dict[key][0] < max1:\n",
    "            max3 = TFIDF_dict[key][0]\n",
    "            max3Sent = key;\n",
    "            max_sentences.append((max3Sent, TFIDF_dict[key][1], TFIDF_dict[key][0]))'''\n",
    "        \n",
    "    #return max_sentences\n",
    "    return sorted_TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_words(positions, img):\n",
    "\n",
    "    overlay = img.copy()\n",
    "\n",
    "    img = cv2.imread('harrypotter-p1.jpg')\n",
    "    \n",
    "    for i in range(len(positions)):\n",
    "        ((x, y), (w, h)) = positions[i].position\n",
    "        cv2.rectangle(overlay, (x, y), (w, h), (255, 0, 0), -1)\n",
    "    alpha = 0.4  # Transparency factor.\n",
    "    img_new = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "    r = 600.0 / img_new.shape[1]  # resizing image without loosing aspect ratio\n",
    "    dim = (600, int(img_new.shape[0] * r))\n",
    "    # perform the actual resizing of the image and show it\n",
    "    resized = cv2.resize(img_new, dim, interpolation=cv2.INTER_AREA)\n",
    "    cv2.imshow('img', resized)\n",
    "    cv2.imwrite(\"highlighted-harrypotter-p1.jpg\", resized)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Get data from image\n",
    "    doc, word_boxes = getTextFromImage('harrypotter-p1.jpg')\n",
    "    doc = doc.replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \").replace(\"Mr.\", \"Mister\").replace(\"Mrs.\", \"Missus\")\n",
    "    \n",
    "    #Get tfidf dictionary\n",
    "    TFIDF = tfidf(doc)\n",
    "    \n",
    "    #Get top n sentences, where n is len(all_sentences)//3\n",
    "    n = len(TFIDF)//3\n",
    "    sentences = []\n",
    "    for sentence in TFIDF:\n",
    "        sentences.append(sentence[1][-1])\n",
    "    top_n = sentences[:n]\n",
    "    top_n.sort()\n",
    "\n",
    "    all_sentences = doc.strip().split(\". \")\n",
    "    \n",
    "    #Get starting and ending indices to make it easy to get the coordinates\n",
    "    k = 0\n",
    "    indices = []\n",
    "    for i in range(len(all_sentences)):\n",
    "        indices.append((k, k + len(all_sentences[i].split(\" \"))))\n",
    "        k += len(all_sentences[i].split(\" \"))\n",
    "        \n",
    "    #Get the positions for all the words in the important sentences list\n",
    "    positions = []\n",
    "    for i in range(len(top_n)):\n",
    "        positions += word_boxes[indices[top_n[i]][0]:indices[top_n[i]][1]]\n",
    "        \n",
    "    img = cv2.imread('harrypotter-p1.jpg')\n",
    "    highlight_words(positions, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
